{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to add the dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import simplejson\n",
    "import calendar\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to downlaod from the databases all the inputs and all the sizes of these inputs (we aren't getting into sql details now).Â "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load ALL_INPUTS db\n",
    "response = urllib2.urlopen(\"http://showmeyourcode.org/get_all_inputs.php\")\n",
    "data = simplejson.load(response)\n",
    "\n",
    "#load sizes\n",
    "sizes = urllib2.urlopen(\"http://showmeyourcode.org/getSizes.php\")\n",
    "sizesData = simplejson.load(sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to keep the size data in two different lists just to use them easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sizeIds=[]\n",
    "sizeValues=[]\n",
    "for i in range(len(sizesData['results'])):\n",
    "\tsizeIds.append(sizesData['results'][i]['id'])\n",
    "\tsizeValues.append(sizesData['results'][i]['size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to do the same with the input data (X). We store the entries in allInputs list, the database times on allTimes list and the ids in allIds list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for X\n",
    "allInputs = []\n",
    "allTimes = []\n",
    "allIds = []\n",
    "\n",
    "\n",
    "lengthOfList = len(data['results'])\n",
    "\n",
    "#set the values\n",
    "for i in range(lengthOfList):\n",
    "\tallIds.append(data['results'][i]['stores_id'])\n",
    "\tallInputs.append(data['results'][i]['input'])\n",
    "\tallTimes.append(data['results'][i]['time_entered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For allTimes we need to keep only the day of the week and also the hour (not the minutes or the seconds). We save this info at allDaysOfWeek and allHours lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allDaysOfWeek = []\n",
    "allHours = []\n",
    "for i in range(lengthOfList):\n",
    "\tallDaysOfWeek.append(calendar.weekday(int(allTimes[i][0:4]), int(allTimes[i][5:7]), int(allTimes[i][8:10])))\n",
    "\tallHours.append(int(allTimes[i][11:13]))\n",
    "\n",
    "#free some space\n",
    "allTimes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create the X (input) matrix keeping the day of week, the our and the id of the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input to neural net [allDaysOfWeek[i], allHours[i], allIds[i]] ..done\n",
    "X = []\n",
    "for i in range(lengthOfList):\n",
    "\tX.append( [  float(allDaysOfWeek[i]),float(allHours[i]),float(allIds[i])] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we have to create the y values as one hot enctyption. We are using sizes to calculate that (because \"almost empty\" for example is different in a store of 50 tables than in a store of 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create y value (real value) as one hot vector [empty, almost empty, half, almost full, full]\n",
    "y = []\n",
    "for i in range(lengthOfList):\n",
    "\tsize = sizeValues[sizeIds.index(allIds[i])]\n",
    "\tsize = float(size)\n",
    "\tallInputs[i] = float(allInputs[i])\n",
    "\tif allInputs[i] >= (size - 0.05*size):\n",
    "\t\ty.append([1,0,0,0,0]) #empty\n",
    "\telif allInputs[i] >= (size - 0.25*size):\n",
    "\t\ty.append([0,1,0,0,0]) #almost empth\n",
    "\telif allInputs[i] >= (size - 0.5*size):\n",
    "\t\ty.append([0,0,1,0,0]) #half \n",
    "\telif allInputs[i] >= (size - 0.85*size):\n",
    "\t\ty.append([0,0,0,1,0]) #almost full\n",
    "\telse:\n",
    "\t\ty.append([0,0,0,0,1]) #full\n",
    "        \n",
    "#convert X,y to np array        \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now the X and the y. We can start the creation of our neural net. First, we define two functions. A simple function that return the softmax and a second function that return the sigmoid (for the forward propagation) or the derivitive of the sigmoid (for the back propagation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#neural network staff \n",
    "np.random.seed(1)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "\tif (deriv==True):\n",
    "\t\treturn x*(1-x)\n",
    "\n",
    "\treturn 1/(1+np.exp(-x))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have to define our synapses. The neural net has 3 inputs, 10 nurons as hiden layer and 5 outputs. So the synapses matrix between input and hiden layer is a 2x10 matrix and the second one is a 10x5 matrix. We define the synapses matrices filling them with random data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn0 = 2*np.random.random((3,10)) - 1\n",
    "syn1 = 2*np.random.random((10,5)) - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the magic is happening. The training itself. Layer 0 is the input data obviously. We calculate the activation of the second layer doing the dot product of the input layer with the first sypapses (forward propagation) and we pass then from the sigmoid function. We are doing the same for the second layers and we keep the error (the difference betweeen the current output of the neural net to the real output) in l2_error variable. \n",
    "\n",
    "Using this error we can do backpropagation. We need to find out how we need to change the weights to predict the output correct (to reduce the error). We calculate the deltas using the backprogation rule: multipling the error with the derivitive. Finally we update our sypaptic weights using the calculated deltas. \n",
    "\n",
    "Repeat that for 60K time and hopefully the network will be able to do accurate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training step\n",
    "for j in range(60000):\n",
    "\n",
    "\tl0 = X\n",
    "\n",
    "\tl1 = nonlin(np.dot(l0, syn0))\n",
    "\tl2 = nonlin(np.dot(l1,syn1))\n",
    "\n",
    "\tl2_error = y - l2 \n",
    "\t\n",
    "\tif (j % 10000) == 0: \n",
    "\t\tprint (\"Error:\" + str(np.mean(np.abs(l2_error))) )\n",
    "\n",
    "\tl2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "\n",
    "\tl1_error = np.dot(l2_delta,syn1.T) # np.dot(l2_delta*syn1.T)\n",
    "\tl1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "\t#update weights (gradient descent)\n",
    "\tsyn1 += np.dot(l1.T,l2_delta)\n",
    "\n",
    "\tsyn0 += np.dot(l0.T,l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the very end we save the weights in a pickle. This way we just use them for the prediction. We don't retrain the whole thing (it is a slow process). We chooce to retrain that in every 10 real entries, but this is defines on php code and we are not getting there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "#Saving weights in a file\n",
    "with open('weights.pickle', 'w') as f:  \n",
    "\tpickle.dump([syn0, syn1], f)\n",
    "\n",
    "print \"done\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
